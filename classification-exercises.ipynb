{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8feffb1d",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "In a jupyter notebook, classification_exercises.ipynb, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. \n",
    "- Create a pandas dataframe, df_iris, from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from acquire import get_iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b11b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the db\n",
    "df_iris = get_iris_data()\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9703e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the first 3 rows:\n",
    "df_iris.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of rows and columns (shape)\n",
    "df_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e0a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the column names\n",
    "df_iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the data type of each column\n",
    "df_iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54df264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the summary statistics for each of the numeric variables\n",
    "df_iris_t= df_iris.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ca0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris_t= df_iris['range'] = df_iris['max'] - df_iris['min']\n",
    "df_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb1576",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "Read the Table1_CustDetails table from your spreadsheet exercises google sheet into a dataframe named df_google_sheets.\n",
    "\n",
    "Make sure that the spreadsheet is publicly visible under your sharing settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acquire import google_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets=google_sheets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ebc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the first 100 rows to a new dataframe, df_google_sheets_sample\n",
    "df_google_sheets_sample = df_google_sheets.head(100)\n",
    "df_google_sheets_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6bae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of rows of your original dataframe\n",
    "df_google_sheets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32822c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first 5 column names\n",
    "df_google_sheets.iloc[:0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR....\n",
    "df_google_sheets.columns[:5] #slicing data with the []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the column names that have a data type of object\n",
    "df_google_sheets.dtypes[df_google_sheets.dtypes == \"object\"].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a94a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR..\n",
    "df_google_sheets.select_dtypes(include='object').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bac3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets.select_dtypes(include='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the range for each of the numeric variables.\n",
    "float=df_google_sheets.dtypes[df_google_sheets.dtypes == \"float\"].index.values\n",
    "\n",
    "float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da043916",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_range = df_google_sheets.monthly_charges.max()-df_google_sheets.monthly_charges.min()\n",
    "total_range = df_google_sheets.total_charges.max()-df_google_sheets.total_charges.min()\n",
    "tenure_range = df_google_sheets.tenure.max()-df_google_sheets.tenure.min()\n",
    "\n",
    "monthly_range,total_range,tenure_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91487b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Another way of doing this:\n",
    "df_google_sheets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b36281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets.describe().loc[['min','max']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_sheets[['max','min']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68433e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_google_sheets['range']= df_google_sheets['max'] - df_google_sheets['min']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d14da",
   "metadata": {},
   "source": [
    "### Exercise 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03bb5c5",
   "metadata": {},
   "source": [
    "Download your spreadsheet exercises google sheet as an excel file (File → Download → Microsoft Excel). Read the Table1_CustDetails worksheet into a dataframe named df_excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when acquiring data (using the acquire.py, remember to import either wholesale of the file OR use the FROM acquire import____df wantied)\n",
    "from acquire import excel_file\n",
    "df_excel = excel_file()\n",
    "df_excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the first 100 rows to a new dataframe, df_excel_sample\n",
    "df_excel_sample=df_excel.head(100)\n",
    "df_excel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of rows of your original dataframe\n",
    "df_excel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf9123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first 5 column names\n",
    "df_excel.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f102b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the column names that have a data type of object\n",
    "df_excel.columns.select_dtypes[include =='object']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4dcd7",
   "metadata": {},
   "source": [
    "### Acquire.py Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33c2e5",
   "metadata": {},
   "source": [
    "#### Exercise One of acquire.py functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85370955",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = acquire.get_titanic_data() ## need to add the data/functions to acquire\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75f3c4",
   "metadata": {},
   "source": [
    "#### Exercise Two of acquire.py function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183af592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180aedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c9e0511",
   "metadata": {},
   "source": [
    "### Prepare Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd47e9",
   "metadata": {},
   "source": [
    "The end product of this exercise should be the specified functions in a python script named prepare.py. Do these in your classification_exercises.ipynb first, then transfer to the prepare.py file.\n",
    "\n",
    "This work should all be saved in your local classification-exercises repo. Then add, commit, and push your changes.\n",
    "\n",
    "<b>Using the Iris Data:</b>\n",
    "\n",
    "- Use the function defined in acquire.py to load the iris data. ✅ \n",
    "\n",
    "- Drop the species_id and measurement_id columns. ✅ \n",
    "\n",
    "- Rename the species_name column to just species. ✅ \n",
    "\n",
    "- Create dummy variables of the species name. ✅ \n",
    "\n",
    "- Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied. ✅ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "import pandas as pd\n",
    "df = acquire.get_iris_data()\n",
    "df.head()\n",
    "\n",
    "#Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b9274",
   "metadata": {},
   "source": [
    "#### Takeaways: Next for Cleaning\n",
    "- Drop the Unnamed, species_id and measurement_id columns\n",
    "- Rename the species_name column to just 'species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns\n",
    "columns_to_drop = ['species_id']\n",
    "#rename df so it does not interfere with og df\n",
    "sepal_data = df.drop(columns=columns_to_drop) \n",
    "sepal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1861fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_data.rename(columns = {'species_name':'species'}, inplace = True)\n",
    "sepal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcf9d5",
   "metadata": {},
   "source": [
    "#### Takeaways, and next steps:\n",
    "- Create dummy variables of the species name.\n",
    "\n",
    "- Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy variables of the species name\n",
    "dummy_df = pd.get_dummies(sepal_data[['species']], dummy_na=False, drop_first=[True])\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62203894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate my dummy_df to my data\n",
    "sepal_data1= pd.concat([sepal_data, dummy_df], axis=1)\n",
    "sepal_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_iris_data()\n",
    "clean_df= prep_iris(df)\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9ccf1",
   "metadata": {},
   "source": [
    "#### Using the Titanic Dataset\n",
    "\n",
    "Use the function defined in acquire.py to load the Titanic data.\n",
    "\n",
    "- Drop any unnecessary, unhelpful, or duplicated columns.\n",
    "\n",
    "- Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "- Create a function named prep_titanic that accepts the raw titanic data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ddcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acquire import new_titanic_data\n",
    "df = new_titanic_data()\n",
    "\n",
    "#Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72026f6e",
   "metadata": {},
   "source": [
    "###Takeaways: \n",
    "- Drop columns class, embarked and passenger_id\n",
    "- Drop deck: not needed/too many NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns\n",
    "columns_to_drop=['embarked', 'class', 'passenger_id', 'deck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a79f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns=columns_to_drop) #saved as a new variable so as not to mess up original data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beeab32",
   "metadata": {},
   "source": [
    "#Encoding: Turning categorical columns into Boolean\n",
    "- sex, embark_town (so both are easier to pull data)\n",
    "    -we concat to the add to our data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(data[['sex', 'embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "data= pd.concat([data, dummy_df], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function named prep_titanic that accepts the raw titanic data, \n",
    "#and returns the data with the transformations above applied.\n",
    "def clean_titanic_data(data):\n",
    "    '''\n",
    "    Takes in a titanic dataframe and returns a clean dataframe\n",
    "    Arguments: df - a pandas dataframe with the expected feature names and columns\n",
    "    Return: clean_df - a dataframe with the cleaning operations performed on it\n",
    "    '''\n",
    "    #Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    #Drop columns\n",
    "    columns_to_drop=['embarked', 'class', 'passenger_id', 'deck']\n",
    "    data = df.drop(columns=columns_to_drop) #saved as a new variable so as not to mess up original data\n",
    "    #Encoding categorical variables\n",
    "    dummy_df = pd.get_dummies(data[['sex', 'embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "    data= pd.concat([data, dummy_df], axis=1)\n",
    "    return data.drop(columns=['sex','embark_town'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90202856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.new_titanic_data()\n",
    "prep_titanic = clean_titanic_data(data)\n",
    "prep_titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e9f37",
   "metadata": {},
   "source": [
    "### Using the Telco dataset:\n",
    "\n",
    "Use the function defined in acquire.py to load the Telco data.\n",
    "\n",
    "- Drop any unnecessary, unhelpful, or duplicated columns. This could mean dropping foreign key columns but keeping the corresponding string values, for example.\n",
    "\n",
    "- Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "- Create a function named prep_telco that accepts the raw telco data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de04552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from acquire import google_sheets\n",
    "df = google_sheets()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.contract_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301907cb",
   "metadata": {},
   "source": [
    "#### Inspect and Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb03ecf",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- create new column with strings for contract_type (helps understand what is being encoded later on)\n",
    "- Encode gender, dependents and partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c742908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column to describe contract types\n",
    "df[\"contract_type\"].replace({0: \"month_to_month\", 1: \"one_year\", 2: \"two_years\"}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding columns:\n",
    "dummy_df = pd.get_dummies(df[['gender', 'dependents', 'partner', 'contract_type', 'payment_type']], dummy_na=False, drop_first=[True, True, True, True])\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat to original\n",
    "df_telco = pd.concat([df, dummy_df], axis=1)\n",
    "df_telco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_telco_data(df):\n",
    "    '''\n",
    "    Takes in the Telco dataset and cleans & prepares for test, validation and training purposes.\n",
    "    '''\n",
    "    #create new column to describe contract types\n",
    "    df[\"contract_type\"].replace({0: \"month_to_month\", 1: \"one_year\", 2: \"two_years\"}, inplace=True)\n",
    "    #drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    #encoding columns:\n",
    "    dummy_df = pd.get_dummies(df[['gender', 'dependents', 'partner', 'contract_type', 'payment_type']], dummy_na=False, drop_first=[True, True, True, True])\n",
    "    #concat to original\n",
    "    df_telco = pd.concat([df, dummy_df], axis=1)\n",
    "    return df_telco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a43af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function named prep_telco that accepts the raw telco data, \n",
    "#and returns the data with the transformations above applied.\n",
    "import acquire\n",
    "df = acquire.google_sheets()\n",
    "prep_telco= clean_telco_data(df)\n",
    "prep_telco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44840116",
   "metadata": {},
   "source": [
    "### Exploratory Analysis Exercises:\n",
    "\n",
    "#### Section 1 - iris_db: Using iris data from our mySQL server and the methods used in the lesson above:\n",
    "\n",
    "- Acquire, prepare & split your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import acquire\n",
    "import prepare\n",
    "\n",
    "df= acquire.get_iris_data()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.prep_iris(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
